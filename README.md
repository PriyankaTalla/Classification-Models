## Below are the list of basic to advanced classification models we have in this repo

1. Logistic Regression
Concepts: Binary classification, sigmoid function.
Mathematical Topics:
Probability: Odds, log-odds.
Optimization: Maximum likelihood estimation (MLE), gradient descent.
Example: Predicting whether a customer will purchase a product based on age and income.

2. K-Nearest Neighbors (KNN)
Concepts: Instance-based learning, distance metrics.
Mathematical Topics:
Statistics: Voting mechanisms, probability.
Mathematics: Euclidean distance, Manhattan distance.
Example: Classifying whether an email is spam based on the frequency of certain words.

3. Decision Trees
Concepts: Recursive partitioning, tree structure.
Mathematical Topics:
Information Theory: Entropy, information gain.
Statistics: Gini impurity, classification error.
Example: Predicting whether a patient has a certain disease based on symptoms.

4. Naive Bayes
Concepts: Conditional independence, Bayes' theorem.
Mathematical Topics:
Probability: Joint, marginal, and conditional probabilities.
Statistics: Gaussian distribution, multinomial distribution.
Example: Classifying text documents into categories.

5. Support Vector Machines (SVM)
Concepts: Hyperplanes, margin maximization.
Mathematical Topics:
Linear Algebra: Dot products, vector norms.
Optimization: Quadratic programming, Lagrange multipliers.
Example: Classifying images of handwritten digits.

6. Random Forests
Concepts: Ensemble learning, bootstrap aggregation (bagging).
Mathematical Topics:
Statistics: Bootstrapping, variance reduction.
Information Theory: Entropy, information gain.
Example: Predicting loan approval based on applicant information.

7. Gradient Boosting Machines (GBM)
Concepts: Boosting, additive models.
Mathematical Topics:
Optimization: Gradient descent, loss functions.
Statistics: Bias-variance tradeoff.
Example: Predicting customer churn based on usage patterns.

8. AdaBoost
Concepts: Adaptive boosting, weight updating.
Mathematical Topics:
Optimization: Weighted errors, exponential loss.
Statistics: Weak learners, ensemble methods.
Example: Classifying credit card transactions as fraudulent or not.

9. XGBoost
Concepts: Extreme gradient boosting, regularization.
Mathematical Topics:
Optimization: Regularized objective functions, gradient boosting.
Statistics: Feature importance, shrinkage.
Example: Predicting loan default risk based on borrower features.

10. LightGBM
Concepts: Light gradient boosting, histogram-based methods.
Mathematical Topics:
Optimization: Leaf-wise tree growth, gradient boosting.
Statistics: Feature importance, L1 and L2 regularization.
Example: Classifying customer reviews as positive or negative.

11. CatBoost
Concepts: Categorical boosting, ordered boosting.
Mathematical Topics:
Optimization: Gradient boosting, symmetric trees.
Statistics: Handling categorical variables, L2 regularization.
Example: Predicting customer churn in a telecom dataset.
